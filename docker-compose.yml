services:
  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    expose:
      - "11434"
    environment:
      - OLLAMA_NUM_PARALLEL=${OLLAMA_NUM_PARALLEL}
      - OLLAMA_NUM_THREADS=${OLLAMA_NUM_THREADS}
      - OLLAMA_MAX_LOADED_MODELS=1
    volumes:
      - ./ollama:/root/.ollama
    healthcheck:
      test: ["CMD-SHELL", "ollama list >/dev/null 2>&1 || wget -q -O- http://127.0.0.1:11434/ >/dev/null 2>&1 || exit 1"]
      interval: 10s
      timeout: 3s
      retries: 15
      start_period: 30s
    restart: unless-stopped
    networks:
      - ai_net

  init-ollama:
    image: curlimages/curl:8.10.1
    container_name: init-ollama
    depends_on:
      ollama:
        condition: service_started
    command:
      [
        "sh","-lc",
        "echo 'Pulling model ${OLLAMA_MODEL}' && \
         curl -fsS -X POST http://ollama:11434/api/pull \
         -H 'Content-Type: application/json' \
         -d '{\"name\":\"'\"${OLLAMA_MODEL}\"'\"}' && \
         echo 'Model ready'"
      ]
    restart: "no"
    networks:
      - ai_net

  enricher:
    build: ./enricher
    container_name: enricher
    depends_on:
      ollama:
        condition: service_healthy
      init-ollama:
        condition: service_completed_successfully
    environment:
      - OLLAMA_BASE_URL=http://ollama:11434
      - OLLAMA_MODEL=${OLLAMA_MODEL}
      - OLLAMA_NUM_CTX=${OLLAMA_NUM_CTX}
    expose:
      - "8090"
    healthcheck:
      test: ["CMD-SHELL", "curl -sf http://127.0.0.1:8090/health > /dev/null || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 15
    restart: unless-stopped
    networks:
      - ai_net

networks:
  ai_net:
    driver: bridge